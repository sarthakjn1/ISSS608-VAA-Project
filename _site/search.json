[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA-Project",
    "section": "",
    "text": "This is a Quarto Website demonstrating Project Work for ISSS608 - Visual Analytics and Applications. This website addresses problem pertaining to 2024 Vast Challenge Mini Challenge 2.\nLinks below:\n2024 VAST Challenge :\nhttps://vast-challenge.github.io/2024/MC2.html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project/Proposal.html",
    "href": "Project/Proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "2024 VAST Challenge \nhttps://vast-challenge.github.io/2024/MC2.html \n\n\n   Visual Analytics for Monitoring and Preventing Illegal Fishing Activities in Oceanus \n\n\n\nOceanus the island nation’s economy thrives on its fishing industry. The recent illegal fishing scandal involving SouthSeafood Express Corp has not only disrupted this vital sector but has also exposed significant gaps in monitoring and regulating fishing activities. By harnessing the power of visual analytics, this project aims to transform raw data into actionable insights, to unearth hidden patterns and to create a system capable of safeguarding Oceanus against future illegal activities. \n\n\n\nThis project addresses the critical challenge of detecting and predicting illegal fishing operations in Oceanus.  \nThe lack of accurate port records has led to a significant gap in tracking the true source and distribution of fish, making it difficult to enforce regulations and ensure sustainability.  \nThis project aims to fill these critical gaps by developing advanced visual analytics tools that can effectively integrate and analyze diverse data sources, offering a more reliable solution to monitor and combat illegal fishing activities.” \n\n\n\nhttps://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00331-z \nThe study focuses on analyzing the global liner shipping network through various network representations and methodologies. The authors address the limitations of previous approaches which used shortest paths in fully connected undirected graphs, which could misrepresent the actual connectivity and paths in the liner shipping service route data. Instead, they propose and evaluate three different representations of the network: directed co-route graph, undirected co-route graph, and the path graph. \nKey innovations in the study include the development of a new method for constructing minimum-route paths from liner shipping data, which provides a more accurate reflection of route connectivity and distances than previous models. The researchers also introduce a modified betweenness centrality measure called route betweenness, which they use to analyze the network’s structure and the roles of various nodes and edges more effectively. \nhttps://link.springer.com/chapter/10.1007/978-3-030-61852-0_10 \nThe study investigates why certain ports are better connected than others. It explores the hierarchical nature of the network. Various measures of node connectivity are employed, all of which underscore an uneven distribution of traffic across the network. \nThe research delves into how cargo specialization or diversity influences the network’s structure. This aspect is analyzed through the lenses of multiplexity and assortativity, assessing whether nodes (ports) tend to diversify their activities or specialize. The analysis is conducted across two main layers—cargo and bulk—with findings indicating that larger ports and their connections tend to show greater diversification. \nThe final area of investigation focuses on the geographical patterns that underlie the distribution of maritime flows. The study examines how physical distance affects connectivity and the formation of subnetworks within the overall network. \n\n\n\nWe plan to analyze illegal fishing behavior and identify the anomalies in vessel movement and behavior through the R Shiny Web app. The application will have the following tabs: \n\nVessel Movement: A tab that can visually indicate the path of a vessel historically. This can help identify any seasonal trends or deviations of a vessel from its usual path. \n\n\n\nCargo and Vessel Matching: Matching the cargo visually to a vessel, the fish it contains, the date of delivery, and the city it was delivered to. This can be analyzed by the construction of a knowledge graph. \n\n\n\nSouthSeaFoodCorp Vessels: Suspicious behavior of SouthSeaFoodCorp Express Vessels. Comparison of their vessel trajectory and fishing contents with other vessels. \n\n\n\nIllegal activities after SouthSeaFoodCorp Express was caught: Behavior of the Shipping community after SouthSeaFoodCorp Express was caught. \n\nCreation of Knowledge Graph to identify Cargo Vessel Match: \nWe have Cargo Transaction Records, Vessel Transponder Ping Records, Harbor Report Records and details of vessels and Locations. We can use this information to develop a possible cargo vessel mapping. Then we plan to do some data transformation to represent this visually through a Knowledge Graph. \nVessel Movement: \nWe can identify anomalies in vessel movement using DBSCAN Clustering. We already have coordinates of the cities and navigation points. Plotting out these can help us track vessel movements with time, identify seasonality and anomalies. \nKnowledge Graph for zooming in on SouthSeaFoodCorp Vessels: \nWe can plot a graph to inspect activity of SouthSeaFoodCorp Vessels. This can be used to identify the fishing done, the timelines, the places of fishing and possibly isolate out the illegal activities. Once the event of SouthSeaFoodCorp being caught is identified we plan to do a temporal analysis for analyzing how other organizations changed their behavior. \nProjected timeline: \nThe entire project will be done in 4 weeks. \nWeek1: \nMatching cargo to the vessels \nDigging deeper into SouthSeaFoodCorp Vessels \nWeek 2: \nPlotting Vessel Movements to identify seasonality and anomalies.  \nPinpointing the time when SouthSeaFoodCorp was caught and analyzing activities of other companies after that. \nWeek 3: \nIncorporating codes into R Shiny \nIdentifying vessels showing behavior like SouthSeaFoodCorp vessels \nWeek 4: \nTouch up R Shiny dashboard layouts. \nFocus on completing activities remaining from Week 1, 2 & 3. \nElaborate on the kind of illegal activities the vessels are engaging in. \n\n\n\nDiagram 1: EDA to understand the usual fishing spots and regions that are frequented by the suspected vessels \nSouthSeafood Express Corp’s Activities \n\nDiagram 2: Which locations should it not be at? \n\nDiagram 3: Merging geospatial analysis with network analysis \n\nSnappersnatcher7be appears to be frequently visiting areas Nav1 and Nav2, which are located around the Ghoti Preserve. Given the type of fish it typically catches, it should not be near these locations. \nDiagram 4: The other vessel does not seem to demonstrate suspicious activities \n\n\n\n\nSuccessful implementation of this project will deter illegal fishing practices through advanced detection capabilities, making it easier to implement regulations and promoting sustainable fishing practices."
  },
  {
    "objectID": "Project/Proposal.html#proposal-document-for-visual-analytics-project",
    "href": "Project/Proposal.html#proposal-document-for-visual-analytics-project",
    "title": "Project Proposal",
    "section": "",
    "text": "2024 VAST Challenge \nhttps://vast-challenge.github.io/2024/MC2.html \n\n\n   Visual Analytics for Monitoring and Preventing Illegal Fishing Activities in Oceanus \n\n\n\nOceanus the island nation’s economy thrives on its fishing industry. The recent illegal fishing scandal involving SouthSeafood Express Corp has not only disrupted this vital sector but has also exposed significant gaps in monitoring and regulating fishing activities. By harnessing the power of visual analytics, this project aims to transform raw data into actionable insights, to unearth hidden patterns and to create a system capable of safeguarding Oceanus against future illegal activities. \n\n\n\nThis project addresses the critical challenge of detecting and predicting illegal fishing operations in Oceanus.  \nThe lack of accurate port records has led to a significant gap in tracking the true source and distribution of fish, making it difficult to enforce regulations and ensure sustainability.  \nThis project aims to fill these critical gaps by developing advanced visual analytics tools that can effectively integrate and analyze diverse data sources, offering a more reliable solution to monitor and combat illegal fishing activities.” \n\n\n\nhttps://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00331-z \nThe study focuses on analyzing the global liner shipping network through various network representations and methodologies. The authors address the limitations of previous approaches which used shortest paths in fully connected undirected graphs, which could misrepresent the actual connectivity and paths in the liner shipping service route data. Instead, they propose and evaluate three different representations of the network: directed co-route graph, undirected co-route graph, and the path graph. \nKey innovations in the study include the development of a new method for constructing minimum-route paths from liner shipping data, which provides a more accurate reflection of route connectivity and distances than previous models. The researchers also introduce a modified betweenness centrality measure called route betweenness, which they use to analyze the network’s structure and the roles of various nodes and edges more effectively. \nhttps://link.springer.com/chapter/10.1007/978-3-030-61852-0_10 \nThe study investigates why certain ports are better connected than others. It explores the hierarchical nature of the network. Various measures of node connectivity are employed, all of which underscore an uneven distribution of traffic across the network. \nThe research delves into how cargo specialization or diversity influences the network’s structure. This aspect is analyzed through the lenses of multiplexity and assortativity, assessing whether nodes (ports) tend to diversify their activities or specialize. The analysis is conducted across two main layers—cargo and bulk—with findings indicating that larger ports and their connections tend to show greater diversification. \nThe final area of investigation focuses on the geographical patterns that underlie the distribution of maritime flows. The study examines how physical distance affects connectivity and the formation of subnetworks within the overall network. \n\n\n\nWe plan to analyze illegal fishing behavior and identify the anomalies in vessel movement and behavior through the R Shiny Web app. The application will have the following tabs: \n\nVessel Movement: A tab that can visually indicate the path of a vessel historically. This can help identify any seasonal trends or deviations of a vessel from its usual path. \n\n\n\nCargo and Vessel Matching: Matching the cargo visually to a vessel, the fish it contains, the date of delivery, and the city it was delivered to. This can be analyzed by the construction of a knowledge graph. \n\n\n\nSouthSeaFoodCorp Vessels: Suspicious behavior of SouthSeaFoodCorp Express Vessels. Comparison of their vessel trajectory and fishing contents with other vessels. \n\n\n\nIllegal activities after SouthSeaFoodCorp Express was caught: Behavior of the Shipping community after SouthSeaFoodCorp Express was caught. \n\nCreation of Knowledge Graph to identify Cargo Vessel Match: \nWe have Cargo Transaction Records, Vessel Transponder Ping Records, Harbor Report Records and details of vessels and Locations. We can use this information to develop a possible cargo vessel mapping. Then we plan to do some data transformation to represent this visually through a Knowledge Graph. \nVessel Movement: \nWe can identify anomalies in vessel movement using DBSCAN Clustering. We already have coordinates of the cities and navigation points. Plotting out these can help us track vessel movements with time, identify seasonality and anomalies. \nKnowledge Graph for zooming in on SouthSeaFoodCorp Vessels: \nWe can plot a graph to inspect activity of SouthSeaFoodCorp Vessels. This can be used to identify the fishing done, the timelines, the places of fishing and possibly isolate out the illegal activities. Once the event of SouthSeaFoodCorp being caught is identified we plan to do a temporal analysis for analyzing how other organizations changed their behavior. \nProjected timeline: \nThe entire project will be done in 4 weeks. \nWeek1: \nMatching cargo to the vessels \nDigging deeper into SouthSeaFoodCorp Vessels \nWeek 2: \nPlotting Vessel Movements to identify seasonality and anomalies.  \nPinpointing the time when SouthSeaFoodCorp was caught and analyzing activities of other companies after that. \nWeek 3: \nIncorporating codes into R Shiny \nIdentifying vessels showing behavior like SouthSeaFoodCorp vessels \nWeek 4: \nTouch up R Shiny dashboard layouts. \nFocus on completing activities remaining from Week 1, 2 & 3. \nElaborate on the kind of illegal activities the vessels are engaging in. \n\n\n\nDiagram 1: EDA to understand the usual fishing spots and regions that are frequented by the suspected vessels \nSouthSeafood Express Corp’s Activities \n\nDiagram 2: Which locations should it not be at? \n\nDiagram 3: Merging geospatial analysis with network analysis \n\nSnappersnatcher7be appears to be frequently visiting areas Nav1 and Nav2, which are located around the Ghoti Preserve. Given the type of fish it typically catches, it should not be near these locations. \nDiagram 4: The other vessel does not seem to demonstrate suspicious activities \n\n\n\n\nSuccessful implementation of this project will deter illegal fishing practices through advanced detection capabilities, making it easier to implement regulations and promoting sustainable fishing practices."
  },
  {
    "objectID": "Project/Investigations.html",
    "href": "Project/Investigations.html",
    "title": "Investigations",
    "section": "",
    "text": "# Load necessary R packages using pacman\npacman::p_load(\n  # Spatial Data\n  sf, lwgeom, units, maps,\n  \n  # Data Manipulation\n  dplyr, tidyverse, lubridate, jsonlite, httr,\n  \n  # Visualization\n  ggplot2, tmap, leaflet, viridis, plotly, ggraph, gganimate, gifski, ggrepel,\n  \n  # Network Analysis\n  igraph, tidygraph, visNetwork, graphlayouts\n)"
  },
  {
    "objectID": "Project/Investigations.html#loading-r-packages",
    "href": "Project/Investigations.html#loading-r-packages",
    "title": "Investigations",
    "section": "",
    "text": "# Load necessary R packages using pacman\npacman::p_load(\n  # Spatial Data\n  sf, lwgeom, units, maps,\n  \n  # Data Manipulation\n  dplyr, tidyverse, lubridate, jsonlite, httr,\n  \n  # Visualization\n  ggplot2, tmap, leaflet, viridis, plotly, ggraph, gganimate, gifski, ggrepel,\n  \n  # Network Analysis\n  igraph, tidygraph, visNetwork, graphlayouts\n)"
  },
  {
    "objectID": "Project/Investigations.html#loading-the-data",
    "href": "Project/Investigations.html#loading-the-data",
    "title": "Investigations",
    "section": "1.2 Loading the Data",
    "text": "1.2 Loading the Data\nLoading the mc2 json data which is a directed multigraph consisting of nodes containing entities and edges containing relationships\n\njson_data &lt;- fromJSON(\"data/mc2.json\")\n\n\nLinksNodesExtracting Necessary Features\n\n\nIn this section, we will prepare our links dataset\n\n\nShow the code\nlinks_df &lt;- as_tibble(json_data$links) %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type),\n         date =  date,\n         time =  as.POSIXct(time, format=\"%Y-%m-%dT%H:%M:%OS\", tz=\"UTC\"),\n         ping_date = as.Date(as.POSIXct(time, format=\"%Y-%m-%dT%H:%M:%OS\", tz=\"UTC\"))) %&gt;%\n  select(type, time, dwell, source, target, date, ping_date)\n\n\nmc2_links &lt;- links_df%&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n(), .groups = 'drop') %&gt;%\n  filter(source != target) %&gt;%\n  ungroup()\n\n\nglimpse(links_df)\n\n\nRows: 271,643\nColumns: 7\n$ type      &lt;chr&gt; \"Event.TransportEvent.TransponderPing\", \"Event.TransportEven…\n$ time      &lt;dttm&gt; 2035-09-16 04:06:48, 2035-09-20 05:21:33, 2035-09-28 04:31:…\n$ dwell     &lt;dbl&gt; 115074.79, 412706.32, 286092.88, 327623.95, 243225.35, 10956…\n$ source    &lt;chr&gt; \"City of Haacklee\", \"City of Haacklee\", \"City of Haacklee\", …\n$ target    &lt;chr&gt; \"perchplundererbc0\", \"perchplundererbc0\", \"perchplundererbc0…\n$ date      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ping_date &lt;date&gt; 2035-09-16, 2035-09-20, 2035-09-28, 2035-10-04, 2035-10-15,…\n\n\n\n\nIn this section, we will prepare our nodes dataset\n\n\nShow the code\n# Convert nodes to tibble, modify variable types, and select required columns\nnodes_df &lt;- as_tibble(json_data$nodes) %&gt;%\n  mutate(\n    type_original = type,\n    id = as.character(id), \n    type = as.character(type),\n    type = case_when(\n      type %in% c(\"Entity.Vessel.CargoVessel\", \"Entity.Vessel.Ferry.Cargo\", \"Entity.Vessel.FishingVessel\") ~ \"Entity.Vessel\",\n      TRUE ~ type\n    ),\n    tonnage = as.numeric(as.character(tonnage)),\n    length_overall = as.numeric(as.character(length_overall)), \n    Activities = as.character(Activities), \n    fish_species_present, \n    kind = as.character(kind), \n    flag_country = as.character(flag_country),\n    company = as.character(company),\n    name = as.character(name),\n    Name = as.character(Name)) %&gt;%\n  select(id, date, type,type_original, qty_tons, name, Name, company, flag_country, Activities, tonnage, length_overall, fish_species_present, kind)\n\n\nglimpse(nodes_df)\n\n\nRows: 5,637\nColumns: 14\n$ id                   &lt;chr&gt; \"gadusnspecificatae4ba\", \"piscesfrigus900\", \"pisc…\n$ date                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                 &lt;chr&gt; \"Entity.Commodity.Fish\", \"Entity.Commodity.Fish\",…\n$ type_original        &lt;chr&gt; \"Entity.Commodity.Fish\", \"Entity.Commodity.Fish\",…\n$ qty_tons             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ name                 &lt;chr&gt; \"Cod/Gadus n.specificatae\", \"Birdseye/Pisces frig…\n$ Name                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Haacklee…\n$ company              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ flag_country         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Activities           &lt;chr&gt; \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"…\n$ tonnage              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ length_overall       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ fish_species_present &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, …\n$ kind                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"city\", \"…\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n\n\nwrite_rds(links_df, \"data/rds/links_df.rds\")\nlinks_df &lt;- read_rds(\"data/rds/links_df.rds\")\n\nwrite_rds(mc2_links, \"data/rds/mc2_links.rds\")\nmc2_links &lt;- read_rds(\"data/rds/mc2_links.rds\")\n\nwrite_rds(nodes_df, \"data/rds/nodes_df.rds\")\nnodes_df &lt;- read_rds(\"data/rds/nodes_df.rds\")\n\n\n\nIn this section, we will extract out seperate datasets from our nodes and links datasets\n\nvessels_df &lt;- nodes_df %&gt;%\n  filter(type == \"Entity.Vessel\") %&gt;%\n  select(id, Name, company, flag_country, type) %&gt;%\n  mutate(type = sub(\"Entity.Vessel.\", \"\", type))\ndelivery_report_df &lt;- nodes_df %&gt;% \n  filter(type == \"Entity.Document.DeliveryReport\")  %&gt;% \n  rename(cargo_id = id)%&gt;% \n  select(qty_tons,date,cargo_id)\nfish_df &lt;- nodes_df %&gt;% \n  filter(type == \"Entity.Commodity.Fish\") %&gt;% \n  rename(fish_name = name)\ncity_df &lt;- nodes_df %&gt;% \n  filter(type == \"Entity.Location.City\")%&gt;% \n  rename(city_name = Name)\nlocations_df &lt;- nodes_df %&gt;% \n  filter(type == \"Entity.Location.City\"| type==\"Entity.Location.Point\"| type==\"Entity.Location.Region\") %&gt;% \n  rename(location_name = Name)%&gt;% \nselect(id,location_name)\nharbor_report_df &lt;- links_df %&gt;% \n  filter(type == \"Event.HarborReport\")  %&gt;% \n  select(date,source,target)\ntransponder_ping_df &lt;- links_df %&gt;% \n  filter(type == \"Event.TransportEvent.TransponderPing\") %&gt;% \n  select(time,dwell,source,target,ping_date)\ntransactions_df &lt;- links_df %&gt;% \n  filter(type == \"Event.Transaction\") %&gt;% \n  select(date,source,target)\nfishing_grounds_df &lt;- nodes_df %&gt;% \n  filter(kind == \"Fishing Ground\"|kind == \"Ecological Preserve\") %&gt;% \n  select(id,Name,kind,fish_species_present)"
  },
  {
    "objectID": "Project/Investigations.html#data-cleaning",
    "href": "Project/Investigations.html#data-cleaning",
    "title": "Investigations",
    "section": "1.3. Data Cleaning",
    "text": "1.3. Data Cleaning\n\nLinksNodesGeographical Information\n\n\nWe discovered that the columns “type,” “source,” and “target” are complete, containing no missing values. Conversely, the columns “time,” “dwell,” “date,” and “ping_date” exhibit numerous missing values. This outcome is anticipated since these columns exclusively hold data for specific categories.\n\n# Check for columns with missing values\ncolSums(is.na(links_df))\n\n     type      time     dwell    source    target      date ping_date \n        0     13101     13101         0         0    258542     13101 \n\n\n\n\nShow the code\nsummary(links_df)\n\n\n     type                time                            dwell         \n Length:271643      Min.   :2035-02-01 00:00:00.00   Min.   :       0  \n Class :character   1st Qu.:2035-04-17 13:33:02.35   1st Qu.:    4695  \n Mode  :character   Median :2035-06-28 19:34:55.25   Median :    6287  \n                    Mean   :2035-06-30 22:13:03.65   Mean   :   19775  \n                    3rd Qu.:2035-09-13 13:44:34.00   3rd Qu.:   12101  \n                    Max.   :2035-11-30 00:00:00.00   Max.   :28735323  \n                    NA's   :13101                    NA's   :13101     \n    source             target              date             ping_date         \n Length:271643      Length:271643      Length:271643      Min.   :2035-02-01  \n Class :character   Class :character   Class :character   1st Qu.:2035-04-17  \n Mode  :character   Mode  :character   Mode  :character   Median :2035-06-28  \n                                                          Mean   :2035-06-30  \n                                                          3rd Qu.:2035-09-13  \n                                                          Max.   :2035-11-30  \n                                                          NA's   :13101       \n\n\nNext, we ensure there are no duplicated rows\n\n\nShow the code\nlinks_df[duplicated(links_df),]\n\n\n# A tibble: 790 × 7\n   type               time   dwell source                target date  ping_date\n   &lt;chr&gt;              &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt; &lt;date&gt;   \n 1 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 2 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 3 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 4 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 5 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 6 Event.HarborReport NA        NA wavewranglerc2d       City … 2035… NA       \n 7 Event.HarborReport NA        NA yellowfintunataker08b City … 2035… NA       \n 8 Event.HarborReport NA        NA webigailba7           City … 2035… NA       \n 9 Event.HarborReport NA        NA webigailba7           City … 2035… NA       \n10 Event.HarborReport NA        NA webigailba7           City … 2035… NA       \n# ℹ 780 more rows\n\n\nLet’s try to understand how our links data is categorized into. It seems there are three categories of data as shown below.\n\n\nShow the code\nunique_type &lt;- unique(links_df$type)\nprint(unique_type)\n\n\nDefine a function to count and print unique categories for a given column.\n\n\nShow the code\ncount_unique_categories &lt;- function(data, column_name) {\n  cat(\"**\", column_name, \"**\\n\", sep = \"\")\n  category_counts &lt;- table(data[[column_name]])\n  sorted_counts &lt;- sort(category_counts, decreasing = TRUE)\n  print(sorted_counts)\n}\n\n\n\n\nLet’s take a look into our nodes dataframe.\n\n\nShow the code\nglimpse(nodes_df)\n\n\nRows: 5,637\nColumns: 14\n$ id                   &lt;chr&gt; \"gadusnspecificatae4ba\", \"piscesfrigus900\", \"pisc…\n$ date                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                 &lt;chr&gt; \"Entity.Commodity.Fish\", \"Entity.Commodity.Fish\",…\n$ type_original        &lt;chr&gt; \"Entity.Commodity.Fish\", \"Entity.Commodity.Fish\",…\n$ qty_tons             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ name                 &lt;chr&gt; \"Cod/Gadus n.specificatae\", \"Birdseye/Pisces frig…\n$ Name                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Haacklee…\n$ company              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ flag_country         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Activities           &lt;chr&gt; \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"…\n$ tonnage              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ length_overall       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ fish_species_present &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, …\n$ kind                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"city\", \"…\n\n\nAgain, it’s hard to make sense if these missing values are actually important due ot the nature of the data. We shall handle it as it goes. However, the main columns we are interested in id and type are not missing any data.\n\n\nShow the code\n# Check for columns with missing values\ncolSums(is.na(nodes_df))\n\n\n                  id                 date                 type \n                   0                  330                    0 \n       type_original             qty_tons                 name \n                   0                  330                 5627 \n                Name              company         flag_country \n                5317                 5458                 5341 \n          Activities              tonnage       length_overall \n                   0                 5359                 5354 \nfish_species_present                 kind \n                   0                 5613 \n\n\nThere does not seem to be any whole duplicate rows.\n\n\nShow the code\nnodes_df[duplicated(nodes_df),]\n\n\n# A tibble: 0 × 14\n# ℹ 14 variables: id &lt;chr&gt;, date &lt;chr&gt;, type &lt;chr&gt;, type_original &lt;chr&gt;,\n#   qty_tons &lt;dbl&gt;, name &lt;chr&gt;, Name &lt;chr&gt;, company &lt;chr&gt;, flag_country &lt;chr&gt;,\n#   Activities &lt;chr&gt;, tonnage &lt;dbl&gt;, length_overall &lt;dbl&gt;,\n#   fish_species_present &lt;list&gt;, kind &lt;chr&gt;\n\n\nIt seems we have many types of nodes even after grouping more of the Vessel types into our generic “Entity.Vessel”. It seems “Entity.Commodity.Fish”,‘Entity.Vessel’, ‘Entity.Location.Point’, ‘Entity.Location.City’, ‘Entity.Location.Region’ will be important for us.\n\n\nShow the code\nunique_type &lt;- unique(nodes_df$type)\nprint(unique_type)\n\n\n [1] \"Entity.Commodity.Fish\"          \"Entity.Location.City\"          \n [3] \"Entity.Document.DeliveryReport\" \"Entity.Vessel\"                 \n [5] \"Entity.Vessel.Other\"            \"Entity.Vessel.Ferry.Passenger\" \n [7] \"Entity.Vessel.Research\"         \"Entity.Vessel.Tour\"            \n [9] \"Entity.Location.Point\"          \"Entity.Location.Region\"        \n\n\n\n\nShow the code\ncount_unique_categories(nodes_df, 'type') \n\n\n**type**\n\nEntity.Document.DeliveryReport                  Entity.Vessel \n                          5307                            280 \n         Entity.Location.Point          Entity.Commodity.Fish \n                            12                             10 \n          Entity.Location.City         Entity.Location.Region \n                             6                              6 \n            Entity.Vessel.Tour            Entity.Vessel.Other \n                             6                              5 \n Entity.Vessel.Ferry.Passenger         Entity.Vessel.Research \n                             3                              2 \n\n\nShow the code\ncount_unique_categories(nodes_df, 'flag_country') \n\n\n**flag_country**\n\n        Oceanus     Playa Solis         Helixia         Nyxonix      Alverossia \n            177               5               4               4               3 \n       Ariuzima        Coralada        Kethanor      Lumindoria       Orvietola \n              3               3               3               3               3 \n     Utoparadia        Uzifrica        Valtalmo Anderia del Mar       Azurionix \n              3               3               3               2               2 \n      Calabrand        Faraluna       Gavanovia     Isla Solmar       Islavaria \n              2               2               2               2               2 \n      Khamseena       Kondarica        Mawalara        Merigrad      Novarctica \n              2               2               2               2               2 \n    Novarcticaa      Osterivaro     Rio Solovia       Riodelsol    Thessalandia \n              2               2               2               2               2 \n     Utoporiana         Uziland       Zawalinda      Afarivaria       Alverovia \n              2               2               2               1               1 \n   Arreciviento         Arvaros       Arvekalia        Baziuzim      Brindisola \n              1               1               1               1               1 \n    Brindivaria     Coralmarica        Helvoris         Icarnia          Imazam \n              1               1               1               1               1 \n    Islavaragon        Kethilim       Kilivaria      Kondanovia    Kondarivakia \n              1               1               1               1               1 \n     Korvelonia       Kuzalanda        Lumakari       Luminkind   Mango del Oro \n              1               1               1               1               1 \n        Marebak        Marifada      Myriadonia        Nalakond       Nalaloria \n              1               1               1               1               1 \n     Oceanterra  Puerto del Mar      Sirenareef     Solovarossa       Solterrix \n              1               1               1               1               1 \n     Syrithania       Talandria    Vesperlandia        Vientoro       Wysterion \n              1               1               1               1               1 \n    Yggdrasonia        Zambarka \n              1               1 \n\n\nShow the code\ncount_unique_categories(nodes_df, 'kind') \n\n\n**kind**\n\n               buoy                city Ecological Preserve      Fishing Ground \n                 12                   6                   3                   3 \n\n\nThere are 100 companies found in our dataset\n\ncount_unique_companies &lt;- length(unique(nodes_df$company))\ncount_unique_companies\n\n[1] 100\n\n\n\n\nThe code below snippet employs the sf package to read and manipulate GeoJSON data. It uses st_read() to import the GeoJSON file and renames the column “Name” to “id” using rename() from the dplyr package.\n\n\nShow the code\n# Read the GeoJSON file\ngeojson_file &lt;- \"data/Oceanus Information/Oceanus Geography.geojson\"\ngeo_data &lt;- st_read(geojson_file) %&gt;%\n  rename(id = Name)\n\n\nReading layer `Oceanus Geography' from data source \n  `C:\\weipengten\\ISSS608-VAA-Project\\Project\\data\\Oceanus Information\\Oceanus Geography.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 29 features and 7 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -167.0654 ymin: 38.07452 xmax: -163.2723 ymax: 40.67775\nGeodetic CRS:  WGS 84\n\n\nUsing tmap we create a basic plot of our locations we are interested in.\n\n\nShow the code\n# Visualize the geographical data with tmap\ntmap_mode(\"plot\")\n\ntm_shape(geo_data) +\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE) +\n  tmap_style(\"gray\") +\n  tm_shape(geo_data) +\n  tm_dots(col = \"purple\", size = 0.2) +\n  tm_text(text = \"id\", size = 0.6, col = \"black\") +  # Add labels to the locations\n  tm_layout(legend.position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Project/Investigations.html#processing-fish-locations-data-and-visualizing",
    "href": "Project/Investigations.html#processing-fish-locations-data-and-visualizing",
    "title": "Investigations",
    "section": "1.4 Processing fish locations data and visualizing",
    "text": "1.4 Processing fish locations data and visualizing\n\n# Initial cleaning and splitting of fish species data\nfishing_grounds_df &lt;- fishing_grounds_df %&gt;%\n  mutate(fish_species_present = gsub(\"c[(]\", \"\", fish_species_present)) %&gt;%\n  mutate(fish_species_present = gsub(\"\\\"\", \"\", fish_species_present)) %&gt;%\n  mutate(fish_species_present = gsub(\"[)]\", \"\", fish_species_present)) %&gt;%\n  mutate(fish_species_present = strsplit(as.character(fish_species_present), \",\\\\s*\")) %&gt;%\n  unnest(fish_species_present) %&gt;%\n  rename(fishing_location = Name)\n\n# Group by fish species and collect all unique locations into a list\nfinal_fish_locations_df &lt;- fishing_grounds_df %&gt;%\n  group_by(fish_species_present) %&gt;%\n  summarise(fishing_locations = list(unique(fishing_location)), .groups = 'drop')\n\nggplot(fishing_grounds_df, aes(x = fishing_location, y = fish_species_present, fill = kind)) +\n  geom_tile(color = \"white\") +\n  scale_fill_manual(values = c(\"Ecological Preserve\" = \"lightblue\", \"Fishing Ground\" = \"lightgreen\")) +\n  labs(title = \"Fish Distribution in Fishing Locations\",\n       x = \"Fishing Location\",\n       y = \"Fish Species\",\n       fill = \"Location Type\") +\n  theme_minimal() + theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.25, hjust=0.25))\n\n\n\n\nPlot above shows the locations in which various fish varieties are found. Fishing is supposed to be done only in the designated fishing grounds and not in ecological preserves\nInsights: There are some fishes like Sockfish/Pisces foetida,Helenaa/Pisces satis, Offidiaa/Piscis osseus which are found only in ecological preserves. So tracking fishing of these varieties can give leads to illegal fishing happening."
  },
  {
    "objectID": "Project/Investigations.html#processing-transactionsharbor-report-and-delivery-report-data",
    "href": "Project/Investigations.html#processing-transactionsharbor-report-and-delivery-report-data",
    "title": "Investigations",
    "section": "1.5 Processing transactions,harbor report and delivery report data",
    "text": "1.5 Processing transactions,harbor report and delivery report data\n\n# Convert date columns to Date type\ntransactions_df$date &lt;- as.Date(transactions_df$date)\nharbor_report_df$date &lt;- as.Date(harbor_report_df$date)\ndelivery_report_df$date &lt;- as.Date(delivery_report_df$date)\n\n\n\n# Performing the inner join to pick only fishing or cargo vessels\nharbor_report_df &lt;- inner_join(harbor_report_df, vessels_df, by =  c(\"source\" = \"id\")) %&gt;%\n  rename(\n    vessel_name = Name,\n    vessel_company = company,\n    vessel_id=source,\n    city=target,\n    vessel_type=type\n  ) %&gt;% \n  select(date,vessel_id,city,vessel_name,vessel_type,vessel_company,flag_country)\n\n\n\ntransponder_ping_df &lt;- inner_join(transponder_ping_df, vessels_df, by =  c(\"target\" = \"id\"))%&gt;%\n  rename(\n    location = source,\n    vessel_id = target,\n    vessel_name = Name,\n    vessel_company = company,\n    vessel_type=type\n  ) %&gt;% \n  select(time,ping_date,dwell, location,vessel_id,vessel_name,vessel_type,vessel_company,flag_country)\n\n# Group by date, vessel_id, and city, and sum up dwell time\ntransponder_ping_day_sum_df &lt;- transponder_ping_df %&gt;%\n  group_by(ping_date, vessel_id, location,vessel_name,vessel_type,vessel_company) %&gt;%\n  summarise(total_dwell_time = sum(dwell, na.rm = TRUE), .groups = 'drop') %&gt;%\n  # Add filter to exclude records where total_dwell_time is 0\n  filter(total_dwell_time != 0)\n\ntransponder_ping_city_df &lt;- transponder_ping_day_sum_df %&gt;%\n  filter(grepl(\"^City\", location))\n\ntransponder_ping_non_city_df &lt;- transponder_ping_day_sum_df %&gt;%\n  filter(!grepl(\"^City\", location))\n\ntransponder_ping_south_df &lt;- transponder_ping_day_sum_df %&gt;%\n  filter(grepl(\"^SouthSeafood Express Corp\", vessel_company))\n\n# preparing transactions\ntransactions_df&lt;-transactions_df%&gt;% rename(cargo_id = source)\n\n#pivot fish and city as columns\ncity_and_fish_df &lt;- bind_rows(city_df, fish_df)\n\ntransactions_type &lt;- transactions_df %&gt;%\n  left_join(city_and_fish_df, by = c(\"target\" = \"id\")) %&gt;%\n  select(date.x, cargo_id, target, type)\n\ntransactions_cargo &lt;- transactions_type %&gt;%\n  pivot_wider(names_from = type, values_from = target,  id_cols = c(date.x, cargo_id))%&gt;%\n  rename(\n    date=date.x,\n    fish = Entity.Commodity.Fish,\n    city = Entity.Location.City\n  )\n\ntransactions_cargo &lt;- transactions_cargo %&gt;%\n  left_join(fish_df, by = c(\"fish\" = \"id\")) %&gt;%\n  select(date.x, cargo_id, fish, city,fish_name)%&gt;%\nrename(date_of_arrival=date.x,\n       city_of_arrival=city,\n       fish_in_cargo=fish\n       )\n\ntransactions_cargo &lt;- transactions_cargo %&gt;%\n  left_join(delivery_report_df, by = c(\"cargo_id\"=\"cargo_id\",\"date_of_arrival\" = \"date\")) %&gt;%\n  select(date_of_arrival,cargo_id, fish_in_cargo, city_of_arrival,fish_name,qty_tons)"
  },
  {
    "objectID": "Project/Investigations.html#eda",
    "href": "Project/Investigations.html#eda",
    "title": "Investigations",
    "section": "2. EDA",
    "text": "2. EDA"
  },
  {
    "objectID": "Project/Investigations.html#exploring-outlier-data",
    "href": "Project/Investigations.html#exploring-outlier-data",
    "title": "Investigations",
    "section": "2.1 Exploring outlier data",
    "text": "2.1 Exploring outlier data\n\niqr_qty &lt;- IQR(transactions_cargo$qty_tons)\nupper_bound &lt;- quantile(transactions_cargo$qty_tons, 0.75) + 1.5 * iqr_qty\noutliers &lt;- transactions_cargo[transactions_cargo$qty_tons &gt; upper_bound, ]\n# Create a histogram\nhist(transactions_cargo$qty_tons)\n\n# Add outliers as points\npoints(transactions_cargo$qty_tons[transactions_cargo$qty_tons &gt; upper_bound], \n       rep(0, sum(transactions_cargo$qty_tons &gt; upper_bound)), \n       col = \"red\", pch = 16)\n\n\n\n\nInsight : Anomaly qty_tons are mostly greater than 70 tons"
  },
  {
    "objectID": "Project/Investigations.html#plotting-fish-arrivals-over-time",
    "href": "Project/Investigations.html#plotting-fish-arrivals-over-time",
    "title": "Investigations",
    "section": "2.2 Plotting fish arrivals over time",
    "text": "2.2 Plotting fish arrivals over time\n\nmonthly_data &lt;- transactions_cargo %&gt;%\n  mutate(month = format(date_of_arrival, \"%Y-%m\"))\n\n# Convert month to Date type for proper ordering in ggplot2\nmonthly_data$month &lt;- as.Date(paste0(monthly_data$month, \"-01\"))\n\n# Group by month and fish_name, and summarize total quantity in tons\nmonthly_data &lt;- monthly_data %&gt;%\n  group_by(month, fish_name) %&gt;%\n  summarize(total_qty = sum(qty_tons)) %&gt;%\n  ungroup()\n\n# Plot the data\np &lt;- ggplot(monthly_data, aes(x = month, y = total_qty, color = fish_name, group = fish_name)) +\n  geom_line() +\n  labs(title = \"Monthly Fish Transactions\", x = \"Month\", y = \"Quantity in Tons\")\n\nggplotly(p)\n\n\n\n\n\nInsights:\n\nHuge increase in Cod arrivals starting from July. This can be investigated for illegal fishing exceeding allowed limits.\nThere are Salmon cargoes arriving,but cant attribute salmon to fishing grounds or ecological preserves. So it might be unregulated fishing from locations where fishing is not supposed to be done.\nProtected species ( Sockfish/Pisces foetida,Helenaa/Pisces satis, Offidiaa/Piscis osseus ) are arriving in cargo and need to be explored"
  },
  {
    "objectID": "Project/Investigations.html#plotting-protected-fish-quantities-arriving-at-each-city-over-time",
    "href": "Project/Investigations.html#plotting-protected-fish-quantities-arriving-at-each-city-over-time",
    "title": "Investigations",
    "section": "2.3 Plotting protected fish quantities arriving at each city over time",
    "text": "2.3 Plotting protected fish quantities arriving at each city over time\n\ncity_time_analysis &lt;- transactions_cargo %&gt;%\n  mutate(month_of_arrival = format(date_of_arrival, \"%Y-%m\"))\n\n# Convert month to Date type for proper ordering in ggplot2\ncity_time_analysis$month_of_arrival &lt;- as.Date(paste0(city_time_analysis$month_of_arrival, \"-01\"))\n\n# Group by month and fish_name, and summarize total quantity in tons\ncity_time_analysis &lt;- city_time_analysis %&gt;%\n  group_by(month_of_arrival,city_of_arrival,fish_name) %&gt;%\n  summarize(total_qty = sum(qty_tons)) %&gt;%\n  ungroup()\n\n# Example endangered fish list\nendangered_fish &lt;- c(\" Helenaa/Pisces satis\", \"Offidiaa/Piscis osseus\",\"Sockfish/Pisces foetida\")\n\n# Generate a color palette, highlighting endangered species\nfish_colors &lt;- ifelse(city_time_analysis$fish_name %in% endangered_fish, \"red\", \"grey\")  # This creates a vector of 'red' or 'grey'\nunique_fish &lt;- unique(city_time_analysis$fish_name)\ncolors &lt;- setNames(ifelse(unique_fish %in% endangered_fish, \"red\", \"grey\"), unique_fish)\n\n# Add a custom color scale to the plot\np &lt;- ggplot(city_time_analysis, aes(x = month_of_arrival, y = total_qty, fill = fish_name)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = colors) +\n  facet_wrap(~ city_of_arrival, scales = \"free_y\") +\n  labs(title = \"Fish Arrivals by City Over Time\", subtitle = \"Red highlights endangered species\",\n       x = \"Month of Arrival\", y = \"Total Quantity in Tons\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.title = element_text(face = \"bold\"))\n\n# Convert to an interactive plotly plot\ninteractive_plot &lt;- ggplotly(p)\n\ninteractive_plot &lt;- interactive_plot %&gt;%\n  layout(\n    hoverlabel = list(bgcolor = \"white\"),\n    hoverinfo = \"text\",\n    tooltip = c(\"month_of_arrival\", \"qty_tons\", \"fish_name\")\n  )\ninteractive_plot\n\n\n\n\n\n\n3.2 Plotting fish quantities arriving at each city over time\n\n# Aggregate quantity by city, fish type, and month of arrival (assuming you have a date_of_arrival column)\n\ncity_time_analysis &lt;- transactions_cargo %&gt;%\n  mutate(month_of_arrival = lubridate::floor_date(date_of_arrival, \"month\")) %&gt;%\n  group_by(city_of_arrival, fish_name, month_of_arrival) %&gt;%\n  summarize(qty_tons = sum(qty_tons), .groups = 'drop')\n\n\n# Plot with ggplot2\np &lt;- ggplot(city_time_analysis, aes(x = month_of_arrival, y = qty_tons, fill = fish_name)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  facet_wrap(~ city_of_arrival, scales = \"free_y\") +\n  labs(title = \"Fish Arrivals by City Over Time\", x = \"Month of Arrival\", y = \"Total Quantity in Tons\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to an interactive plotly plot\nggplotly(p)\n\n\n\n\n\nInsights:\n\nCod arrival increased significantly in Paackland,South Paackland and Lomark from July\nSignificant consistent increase in Wrasse arrivals in Himark\nSockfish arrived significantly in Hacklee and Himark in November\nOffidiaa arrived significantly in Paackland in November"
  },
  {
    "objectID": "Project/Investigations.html#exploring-activities-of-southseafood-express-corp",
    "href": "Project/Investigations.html#exploring-activities-of-southseafood-express-corp",
    "title": "Investigations",
    "section": "4. Exploring activities of SouthSeafood Express Corp",
    "text": "4. Exploring activities of SouthSeafood Express Corp\n\n4.1 Plotting Monthly Dwell time of vessels belonging to SouthSeafood Express Corp\n\n# Aggregate dwell time by month, city, and vessel_id\ndwell_time_by_month_city_vessel &lt;- transponder_ping_df %&gt;%\n  filter(vessel_company == \"SouthSeafood Express Corp\") %&gt;%\n  mutate(month = format(ping_date, \"%Y-%m\")) %&gt;%\n  group_by(month, location, vessel_id) %&gt;%\n  summarise(monthly_dwell_time = sum(dwell), .groups = 'drop')\n\n# Plotting with faceting by vessel_id\np2 &lt;- ggplot(dwell_time_by_month_city_vessel, aes(y = month, x = monthly_dwell_time, fill = location)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~vessel_id, scales = \"free_y\") +  # Free scales can be set if necessary\n  labs(title = \"Monthly Dwell Time by City for Company A, by Vessel\",\n       y = \"Month\",\n       x = \"Dwell Time\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",axis.text.x = element_text(angle = 90, hjust = 1),\n        strip.text.x = element_text(angle = 0, hjust = 0.5, size = 8))  # Adjust strip text for better readability\n\nggplotly(p2)\n\n\n\n\n\n\nInsights:\n\nSouthSeafood Express Corp seems to have been caught in the month of May as transponder pings stops at May\nVessel snappersnatcher7be seems to have spent considerable time in ecological preserve ‘ghoti preserve’ in april and in wrasse beds in april. The common fish present in both locations in Wrasse.\n\n\n\n4.2 Heatmap of dwell time\n\nggplot(transponder_ping_south_df, aes(x = ping_date, y = location, fill = total_dwell_time)) +\n      geom_tile(color = \"white\") +  # Adds borders to each tile\n      scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Dwell Time\") +  # Color gradient\n      facet_wrap(~ vessel_id, ncol = 1, scales = \"free_y\")+\n      labs(title = \"Heatmap of Vessel Location Dwell Times\", x = \"Date\", y = \"Location\") +\n      theme_minimal() +\n      theme(\n        axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate date labels for clarity\n        axis.title = element_text(size = 12, face = \"bold\")\n      )\n\n\n\n\nInsight:\nVessels seem to be spending time in exit east as well Himark and Lomark."
  },
  {
    "objectID": "Project/Investigations.html#deep-dive-into-southseafood-express-corp",
    "href": "Project/Investigations.html#deep-dive-into-southseafood-express-corp",
    "title": "Investigations",
    "section": "5. Deep dive into SouthSeafood Express Corp",
    "text": "5. Deep dive into SouthSeafood Express Corp"
  },
  {
    "objectID": "Project/Investigations.html#looking-for-similar-patterns-as-southseafood-express-corp",
    "href": "Project/Investigations.html#looking-for-similar-patterns-as-southseafood-express-corp",
    "title": "Investigations",
    "section": "6. Looking for similar patterns as SouthSeafood Express Corp",
    "text": "6. Looking for similar patterns as SouthSeafood Express Corp\nNow Lets investigate the arrival of piscisosseusb6d in City of Paackland in November\n\n\n6.1 Top 10 vessels with highest Dwell time in Ghoti Preserve\nLooking for dwell time in Ghoti Preserve, as piscisosseusb6d is only found in Ghoti Preserve and hence is a protected species.\n\n# Specify hardcoded city and month\nhardcoded_city &lt;- \"Ghoti Preserve\"\nselected_month &lt;- \"Nov\"\n\n# Extract year and month\ntransponder_ping_day_sum_df$year &lt;- year(transponder_ping_day_sum_df$ping_date)\ntransponder_ping_day_sum_df$month &lt;- month(transponder_ping_day_sum_df$ping_date, label = TRUE)\n\n# Summarize total dwell time for each vessel in each city and month\nmonthly_summary_df &lt;- transponder_ping_day_sum_df %&gt;%\n  group_by(year, month, location, vessel_id, vessel_type,vessel_company) %&gt;%\n  summarise(total_dwell_time = sum(total_dwell_time), .groups = 'drop') %&gt;%\n  arrange(location, month)\n\n# Filter to keep only top 5 vessels by dwell time for each city and month\ntop_vessels_df &lt;- monthly_summary_df %&gt;%\n  group_by(year, month, location) %&gt;%\n  slice_max(order_by = total_dwell_time, n = 10, with_ties = FALSE)\n\n\n\n# Filter the data for the hardcoded city and selected month\nfiltered_data &lt;- top_vessels_df %&gt;%\n  filter(location == hardcoded_city, month == selected_month) %&gt;%\n  arrange(desc(total_dwell_time))\n\n# Ensure only top 5 vessels are selected in case of any discrepancy\nfiltered_data &lt;- head(filtered_data, 10)\n\n# Create the Plotly plot\nplot &lt;- plot_ly(filtered_data, y = ~vessel_id, x = ~total_dwell_time, type = 'bar', orientation = 'h',\n                text = ~paste(\"Vessel ID:\", vessel_id, \"&lt;br&gt;Vessel Company:\", vessel_company,\"&lt;br&gt;vessel_type:\", vessel_type),\n                hoverinfo = 'text') %&gt;%\n  layout(title = paste(\"Top 10 Vessels in\", hardcoded_city, \"for\", selected_month),\n         yaxis = list(title = \"Vessel ID\"),\n         xaxis = list(title = \"Total Dwell Time (hours)\"),\n         barmode = 'stack')\n\n# Display the plot\nplot\n\n\n\n\n\nInsights:\n\nbrillbandit0a1,tunataker80c seems to be spending considerable amount of time in Ghoti preserve and is a suspect"
  },
  {
    "objectID": "Project/Investigations.html#conclusion",
    "href": "Project/Investigations.html#conclusion",
    "title": "Investigations",
    "section": "Conclusion",
    "text": "Conclusion\n\nAs per above study,South Sea food seems to have performed illegal fishing at Wrasse beds and seems to also have performed transhipment to evade detection.\nWe are able to use similar patterns for detecting illegal fishing by other vessels.\nUsing all mechanism for detection provided above, it will be possible to watch out for anomalies and then plot path to detect illegal fishing."
  },
  {
    "objectID": "Project/Proposal_original.html",
    "href": "Project/Proposal_original.html",
    "title": "Project Proposal",
    "section": "",
    "text": "2024 VAST Challenge \nhttps://vast-challenge.github.io/2024/MC2.html \n\n\n   Visual Analytics for Monitoring and Preventing Illegal Fishing Activities in Oceanus \n\n\n\nOceanus the island nation’s economy thrives on its fishing industry. The recent illegal fishing scandal involving SouthSeafood Express Corp has not only disrupted this vital sector but has also exposed significant gaps in monitoring and regulating fishing activities. By harnessing the power of visual analytics, this project aims to transform raw data into actionable insights, to unearth hidden patterns and to create a system capable of safeguarding Oceanus against future illegal activities. \n\n\n\nThis project addresses the critical challenge of detecting and predicting illegal fishing operations in Oceanus.  \nThe lack of accurate port records has led to a significant gap in tracking the true source and distribution of fish, making it difficult to enforce regulations and ensure sustainability.  \nThis project aims to fill these critical gaps by developing advanced visual analytics tools that can effectively integrate and analyze diverse data sources, offering a more reliable solution to monitor and combat illegal fishing activities.” \n\n\n\nhttps://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00331-z \nThe study focuses on analyzing the global liner shipping network through various network representations and methodologies. The authors address the limitations of previous approaches which used shortest paths in fully connected undirected graphs, which could misrepresent the actual connectivity and paths in the liner shipping service route data. Instead, they propose and evaluate three different representations of the network: directed co-route graph, undirected co-route graph, and the path graph. \nKey innovations in the study include the development of a new method for constructing minimum-route paths from liner shipping data, which provides a more accurate reflection of route connectivity and distances than previous models. The researchers also introduce a modified betweenness centrality measure called route betweenness, which they use to analyze the network’s structure and the roles of various nodes and edges more effectively. \nhttps://link.springer.com/chapter/10.1007/978-3-030-61852-0_10 \nThe study investigates why certain ports are better connected than others. It explores the hierarchical nature of the network. Various measures of node connectivity are employed, all of which underscore an uneven distribution of traffic across the network. \nThe research delves into how cargo specialization or diversity influences the network’s structure. This aspect is analyzed through the lenses of multiplexity and assortativity, assessing whether nodes (ports) tend to diversify their activities or specialize. The analysis is conducted across two main layers—cargo and bulk—with findings indicating that larger ports and their connections tend to show greater diversification. \nThe final area of investigation focuses on the geographical patterns that underlie the distribution of maritime flows. The study examines how physical distance affects connectivity and the formation of subnetworks within the overall network. \n\n\n\nWe plan to analyze illegal fishing behavior and identify the anomalies in vessel movement and behavior through the R Shiny Web app. The application will have the following tabs: \n\nVessel Movement: A tab that can visually indicate the path of a vessel historically. This can help identify any seasonal trends or deviations of a vessel from its usual path. \n\n\n\nCargo and Vessel Matching: Matching the cargo visually to a vessel, the fish it contains, the date of delivery, and the city it was delivered to. This can be analyzed by the construction of a knowledge graph. \n\n\n\nSouthSeaFoodCorp Vessels: Suspicious behavior of SouthSeaFoodCorp Express Vessels. Comparison of their vessel trajectory and fishing contents with other vessels. \n\n\n\nIllegal activities after SouthSeaFoodCorp Express was caught: Behavior of the Shipping community after SouthSeaFoodCorp Express was caught. \n\nCreation of Knowledge Graph to identify Cargo Vessel Match: \nWe have Cargo Transaction Records, Vessel Transponder Ping Records, Harbor Report Records and details of vessels and Locations. We can use this information to develop a possible cargo vessel mapping. Then we plan to do some data transformation to represent this visually through a Knowledge Graph. \nVessel Movement: \nWe can identify anomalies in vessel movement using DBSCAN Clustering. We already have coordinates of the cities and navigation points. Plotting out these can help us track vessel movements with time, identify seasonality and anomalies. \nKnowledge Graph for zooming in on SouthSeaFoodCorp Vessels: \nWe can plot a graph to inspect activity of SouthSeaFoodCorp Vessels. This can be used to identify the fishing done, the timelines, the places of fishing and possibly isolate out the illegal activities. Once the event of SouthSeaFoodCorp being caught is identified we plan to do a temporal analysis for analyzing how other organizations changed their behavior. \nProjected timeline: \nThe entire project will be done in 4 weeks. \nWeek1: \nMatching cargo to the vessels \nDigging deeper into SouthSeaFoodCorp Vessels \nWeek 2: \nPlotting Vessel Movements to identify seasonality and anomalies.  \nPinpointing the time when SouthSeaFoodCorp was caught and analyzing activities of other companies after that. \nWeek 3: \nIncorporating codes into R Shiny \nIdentifying vessels showing behavior like SouthSeaFoodCorp vessels \nWeek 4: \nTouch up R Shiny dashboard layouts. \nFocus on completing activities remaining from Week 1, 2 & 3. \nElaborate on the kind of illegal activities the vessels are engaging in. \n\n\n\nDiagram 1: EDA to understand the usual fishing spots and regions that are frequented by the suspected vessels \nSouthSeafood Express Corp’s Activities \n\nDiagram 2: Which locations should it not be at? \n\nDiagram 3: Merging geospatial analysis with network analysis \n\nSnappersnatcher7be appears to be frequently visiting areas Nav1 and Nav2, which are located around the Ghoti Preserve. Given the type of fish it typically catches, it should not be near these locations. \nDiagram 4: The other vessel does not seem to demonstrate suspicious activities \n\n\n\n\nSuccessful implementation of this project will deter illegal fishing practices through advanced detection capabilities, making it easier to implement regulations and promoting sustainable fishing practices."
  },
  {
    "objectID": "Project/Proposal_original.html#proposal-document-for-visual-analytics-project",
    "href": "Project/Proposal_original.html#proposal-document-for-visual-analytics-project",
    "title": "Project Proposal",
    "section": "",
    "text": "2024 VAST Challenge \nhttps://vast-challenge.github.io/2024/MC2.html \n\n\n   Visual Analytics for Monitoring and Preventing Illegal Fishing Activities in Oceanus \n\n\n\nOceanus the island nation’s economy thrives on its fishing industry. The recent illegal fishing scandal involving SouthSeafood Express Corp has not only disrupted this vital sector but has also exposed significant gaps in monitoring and regulating fishing activities. By harnessing the power of visual analytics, this project aims to transform raw data into actionable insights, to unearth hidden patterns and to create a system capable of safeguarding Oceanus against future illegal activities. \n\n\n\nThis project addresses the critical challenge of detecting and predicting illegal fishing operations in Oceanus.  \nThe lack of accurate port records has led to a significant gap in tracking the true source and distribution of fish, making it difficult to enforce regulations and ensure sustainability.  \nThis project aims to fill these critical gaps by developing advanced visual analytics tools that can effectively integrate and analyze diverse data sources, offering a more reliable solution to monitor and combat illegal fishing activities.” \n\n\n\nhttps://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00331-z \nThe study focuses on analyzing the global liner shipping network through various network representations and methodologies. The authors address the limitations of previous approaches which used shortest paths in fully connected undirected graphs, which could misrepresent the actual connectivity and paths in the liner shipping service route data. Instead, they propose and evaluate three different representations of the network: directed co-route graph, undirected co-route graph, and the path graph. \nKey innovations in the study include the development of a new method for constructing minimum-route paths from liner shipping data, which provides a more accurate reflection of route connectivity and distances than previous models. The researchers also introduce a modified betweenness centrality measure called route betweenness, which they use to analyze the network’s structure and the roles of various nodes and edges more effectively. \nhttps://link.springer.com/chapter/10.1007/978-3-030-61852-0_10 \nThe study investigates why certain ports are better connected than others. It explores the hierarchical nature of the network. Various measures of node connectivity are employed, all of which underscore an uneven distribution of traffic across the network. \nThe research delves into how cargo specialization or diversity influences the network’s structure. This aspect is analyzed through the lenses of multiplexity and assortativity, assessing whether nodes (ports) tend to diversify their activities or specialize. The analysis is conducted across two main layers—cargo and bulk—with findings indicating that larger ports and their connections tend to show greater diversification. \nThe final area of investigation focuses on the geographical patterns that underlie the distribution of maritime flows. The study examines how physical distance affects connectivity and the formation of subnetworks within the overall network. \n\n\n\nWe plan to analyze illegal fishing behavior and identify the anomalies in vessel movement and behavior through the R Shiny Web app. The application will have the following tabs: \n\nVessel Movement: A tab that can visually indicate the path of a vessel historically. This can help identify any seasonal trends or deviations of a vessel from its usual path. \n\n\n\nCargo and Vessel Matching: Matching the cargo visually to a vessel, the fish it contains, the date of delivery, and the city it was delivered to. This can be analyzed by the construction of a knowledge graph. \n\n\n\nSouthSeaFoodCorp Vessels: Suspicious behavior of SouthSeaFoodCorp Express Vessels. Comparison of their vessel trajectory and fishing contents with other vessels. \n\n\n\nIllegal activities after SouthSeaFoodCorp Express was caught: Behavior of the Shipping community after SouthSeaFoodCorp Express was caught. \n\nCreation of Knowledge Graph to identify Cargo Vessel Match: \nWe have Cargo Transaction Records, Vessel Transponder Ping Records, Harbor Report Records and details of vessels and Locations. We can use this information to develop a possible cargo vessel mapping. Then we plan to do some data transformation to represent this visually through a Knowledge Graph. \nVessel Movement: \nWe can identify anomalies in vessel movement using DBSCAN Clustering. We already have coordinates of the cities and navigation points. Plotting out these can help us track vessel movements with time, identify seasonality and anomalies. \nKnowledge Graph for zooming in on SouthSeaFoodCorp Vessels: \nWe can plot a graph to inspect activity of SouthSeaFoodCorp Vessels. This can be used to identify the fishing done, the timelines, the places of fishing and possibly isolate out the illegal activities. Once the event of SouthSeaFoodCorp being caught is identified we plan to do a temporal analysis for analyzing how other organizations changed their behavior. \nProjected timeline: \nThe entire project will be done in 4 weeks. \nWeek1: \nMatching cargo to the vessels \nDigging deeper into SouthSeaFoodCorp Vessels \nWeek 2: \nPlotting Vessel Movements to identify seasonality and anomalies.  \nPinpointing the time when SouthSeaFoodCorp was caught and analyzing activities of other companies after that. \nWeek 3: \nIncorporating codes into R Shiny \nIdentifying vessels showing behavior like SouthSeaFoodCorp vessels \nWeek 4: \nTouch up R Shiny dashboard layouts. \nFocus on completing activities remaining from Week 1, 2 & 3. \nElaborate on the kind of illegal activities the vessels are engaging in. \n\n\n\nDiagram 1: EDA to understand the usual fishing spots and regions that are frequented by the suspected vessels \nSouthSeafood Express Corp’s Activities \n\nDiagram 2: Which locations should it not be at? \n\nDiagram 3: Merging geospatial analysis with network analysis \n\nSnappersnatcher7be appears to be frequently visiting areas Nav1 and Nav2, which are located around the Ghoti Preserve. Given the type of fish it typically catches, it should not be near these locations. \nDiagram 4: The other vessel does not seem to demonstrate suspicious activities \n\n\n\n\nSuccessful implementation of this project will deter illegal fishing practices through advanced detection capabilities, making it easier to implement regulations and promoting sustainable fishing practices."
  },
  {
    "objectID": "Project/Investigations.html#network-analysis-for-southseafood-express-corp",
    "href": "Project/Investigations.html#network-analysis-for-southseafood-express-corp",
    "title": "Investigations",
    "section": "7. Network analysis for SouthSeafood Express Corp",
    "text": "7. Network analysis for SouthSeafood Express Corp\n\n# Filter the dataframe for a specific company \ntransponder_ping_filtered_df &lt;- transponder_ping_day_sum_df %&gt;%\n  filter(vessel_company == \"SouthSeafood Express Corp\")\n\n# Create edge list for the graph\n# Here, 'from' represents the vessel, and 'to' represents the city (location)\nedges_df &lt;- data.frame(\n  from = transponder_ping_filtered_df$vessel_id,\n  to = transponder_ping_filtered_df$location,\n  weight = transponder_ping_filtered_df$total_dwell_time # Use dwell time as weight for edge thickness\n)\n\n# Create the graph object\ngraph &lt;- graph_from_data_frame(edges_df, directed = FALSE)\n\n# Define node types\nV(graph)$type &lt;- ifelse(V(graph)$name %in% transponder_ping_filtered_df$vessel_id, \"vessel\", \"location\")\n\n# Visualize the network with ggraph\nggraph(graph, layout = 'fr') +  \n  geom_edge_link(aes(edge_width = weight), edge_colour = \"grey\") + # Edge thickness based on dwell time\n  geom_node_point(aes(color = type), size = 5) + \n  geom_node_text(aes(label = name), repel = TRUE, size = 3, color = \"black\") +  \n  scale_color_manual(values = c(\"vessel\" = \"orange\", \"location\" = \"plum\")) + \n  theme_void() +  \n  labs(title = \"Network Visualization of Vessel Movements for SouthSeafood Express Corp\")\n\n\n\n\nCentral and High-Activity Locations:\n\nCity of Himark and City of Lomark: These locations have multiple connections and thicker edges, indicating they are key hubs with significant vessel activity or prolonged stays.\nWrasse Beds and Ghoti Preserve: Similarly, these locations are prominent in terms of connections and might represent areas where they are performing illegal fishing\nLocations with Thicker Edges: Locations like “Exit East” connected by thicker edges suggest that these are areas where vessels tend to stay longer, possibly for transshipment activities.\nPaths and Connections: The arrangement of nodes and connections shows how vessels move from one location to another, which locations are frequently connected, and how dense the network is in specific regions.\nThe path plotting show how vessels move from one location to another and which locations are frequently connected\nsnappersnatcher seems to use Exit east for transshipment activities\nroachrobberdb6 seems to use Nav A and Nav E for transshipment activities"
  }
]